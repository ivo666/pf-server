import os
import requests
import pandas as pd
import psycopg2
from datetime import datetime, timedelta
from dotenv import load_dotenv
from psycopg2.extras import execute_values

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
API_TOKEN = os.getenv('API_TOKEN')
BASE_URL = os.getenv('BASE_URL')

headers = {
    "Authorization": f"OAuth {API_TOKEN}",
    "Content-Type": "application/json"
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î –∏–∑ .env —Ñ–∞–π–ª–∞
DB_CONFIG = {
    'host': os.getenv('HOST', 'localhost'),
    'port': os.getenv('PORT', '5432'),
    'database': os.getenv('NAME'),
    'user': os.getenv('USER'),
    'password': os.getenv('PASSWORD')
}

def get_existing_dates_from_db(conn):
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –µ—Å—Ç—å –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –ë–î"""
    print("üóÑÔ∏è –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞—Ç—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
    
    query = "SELECT DISTINCT date FROM rdl.webm_api ORDER BY date"
    
    try:
        with conn.cursor() as cursor:
            cursor.execute(query)
            existing_dates = [row[0].strftime("%Y-%m-%d") for row in cursor.fetchall()]
        print(f"‚úÖ –í –±–∞–∑–µ –Ω–∞–π–¥–µ–Ω–æ {len(existing_dates)} –¥–∞—Ç")
        return existing_dates
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—Ç –∏–∑ –ë–î: {e}")
        return []

def check_date_has_data_in_webmaster(user_id, host_id, target_date):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ –í–µ–±–º–∞—Å—Ç–µ—Ä–µ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã"""
    monitoring_url = f"{BASE_URL}/user/{user_id}/hosts/{host_id}/query-analytics/list"
    
    payload = {
        "limit": 1,
        "text_indicator": "QUERY",
        "filters": {
            "statistic_filters": [{
                "statistic_field": "IMPRESSIONS",
                "operation": "GREATER_THAN",
                "value": "0",
                "from": target_date,
                "to": target_date
            }]
        }
    }
    
    try:
        response = requests.post(monitoring_url, headers=headers, json=payload)
        if response.status_code == 200:
            data = response.json()
            stats_list = data.get('text_indicator_to_statistics', [])
            return len(stats_list) > 0
        else:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–∞—Ç—ã {target_date}: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–∞—Ç—ã {target_date}: {e}")
        return False

def get_missing_dates(conn, user_id, host_id, days_back=20):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–∞—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ë–î"""
    print(f"üìÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days_back} –¥–Ω–µ–π...")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞—Ç—ã –∏–∑ –ë–î
    existing_dates = get_existing_dates_from_db(conn)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–∞—Ç –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
    end_date = datetime.now().date()
    start_date = end_date - timedelta(days=days_back-1)  # -1 —á—Ç–æ–±—ã –≤–∫–ª—é—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –¥–µ–Ω—å
    
    all_dates = []
    current_date = start_date
    while current_date <= end_date:
        all_dates.append(current_date.strftime("%Y-%m-%d"))
        current_date += timedelta(days=1)
    
    print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –í–µ–±–º–∞—Å—Ç–µ—Ä–µ –¥–ª—è {len(all_dates)} –¥–∞—Ç...")
    print(f"   –ü–µ—Ä–∏–æ–¥: {start_date} - {end_date}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –¥–∞—Ç—ã –µ—Å—Ç—å –≤ –í–µ–±–º–∞—Å—Ç–µ—Ä–µ
    available_dates = []
    for i, date_str in enumerate(all_dates, 1):
        if i % 5 == 0 or i == len(all_dates):
            print(f"   –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {i}/{len(all_dates)} –¥–∞—Ç...")
        
        if check_date_has_data_in_webmaster(user_id, host_id, date_str):
            available_dates.append(date_str)
    
    # –ù–∞—Ö–æ–¥–∏–º –¥–∞—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –í–µ–±–º–∞—Å—Ç–µ—Ä–µ, –Ω–æ –Ω–µ—Ç –≤ –ë–î
    missing_dates = [date for date in available_dates if date not in existing_dates]
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   - –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –ø–µ—Ä–∏–æ–¥: {start_date} - {end_date}")
    print(f"   - –ù–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –≤ –í–µ–±–º–∞—Å—Ç–µ—Ä–µ: {len(available_dates)} –¥–∞—Ç")
    print(f"   - –ï—Å—Ç—å –≤ –ë–î: {len(existing_dates)} –¥–∞—Ç")
    print(f"   - –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –ë–î: {len(missing_dates)} –¥–∞—Ç")
    
    if missing_dates:
        print(f"   - –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞—Ç—ã: {', '.join(sorted(missing_dates))}")
    
    return missing_dates

def get_all_urls_for_date(user_id, host_id, target_date):
    """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ URL –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã"""
    monitoring_url = f"{BASE_URL}/user/{user_id}/hosts/{host_id}/query-analytics/list"
    urls = set()
    offset = 0
    limit = 500
    
    while True:
        payload = {
            "offset": offset,
            "limit": limit,
            "text_indicator": "URL",
            "filters": {
                "statistic_filters": [{
                    "statistic_field": "IMPRESSIONS",
                    "operation": "GREATER_THAN",
                    "value": "0",
                    "from": target_date,
                    "to": target_date
                }]
            }
        }
        
        response = requests.post(monitoring_url, headers=headers, json=payload)
        if response.status_code == 200:
            data = response.json()
            stats_list = data.get('text_indicator_to_statistics', [])
            
            if not stats_list:
                break
            
            for item in stats_list:
                url = item.get('text_indicator', {}).get('value', '')
                if url and url != 'N/A':
                    urls.add(url)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –µ—â–µ –¥–∞–Ω–Ω—ã–µ
            if len(stats_list) < limit:
                break
                
            offset += limit
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ URL –∑–∞ {target_date}: {response.status_code}")
            break
    
    return list(urls)

def get_data_for_date_and_url(user_id, host_id, target_date, url, device):
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –¥–∞—Ç—ã, URL –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
    monitoring_url = f"{BASE_URL}/user/{user_id}/hosts/{host_id}/query-analytics/list"
    
    payload = {
        "limit": 500,
        "text_indicator": "QUERY",
        "device_type_indicator": device,
        "filters": {
            "text_filters": [{
                "text_indicator": "URL",
                "operation": "TEXT_MATCH",
                "value": url
            }],
            "statistic_filters": [{
                "statistic_field": "DEMAND",
                "operation": "GREATER_THAN",
                "value": "0",
                "from": target_date,
                "to": target_date
            }]
        }
    }
    
    response = requests.post(monitoring_url, headers=headers, json=payload)
    data_rows = []
    
    if response.status_code == 200:
        data = response.json()
        stats_list = data.get('text_indicator_to_statistics', [])
        
        for item in stats_list:
            text_indicator = item.get('text_indicator', {})
            query_text = text_indicator.get('value', 'N/A')
            
            statistics = item.get('statistics', [])
            
            # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –¥–∞—Ç—ã
            metrics = {}
            for stat in statistics:
                if stat.get('date') == target_date:
                    field = stat.get('field')
                    value = stat.get('value', 0)
                    metrics[field] = value
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ DEMAND > 0
            if metrics and metrics.get('DEMAND', 0) > 0:
                data_row = {
                    'date': target_date,
                    'page_path': url,
                    'query': query_text,
                    'demand': metrics.get('DEMAND', 0),
                    'impressions': metrics.get('IMPRESSIONS', 0),
                    'clicks': metrics.get('CLICKS', 0),
                    'position': metrics.get('POSITION', 0),
                    'device': device
                }
                data_rows.append(data_row)
    
    return data_rows

def save_to_database(df, conn):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    if df.empty:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
    data_tuples = [
        (
            row['date'],
            row['page_path'],
            row['query'],
            row['demand'],
            row['impressions'],
            row['clicks'],
            row['position'],
            row['device']
        )
        for _, row in df.iterrows()
    ]
    
    # SQL –∑–∞–ø—Ä–æ—Å –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    insert_query = """
        INSERT INTO rdl.webm_api (date, page_path, query, demand, impressions, clicks, position, device)
        VALUES %s
    """
    
    try:
        with conn.cursor() as cursor:
            execute_values(cursor, insert_query, data_tuples)
            conn.commit()
        
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î: {len(data_tuples)} –∑–∞–ø–∏—Å–µ–π")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –ë–î: {e}")
        conn.rollback()

def load_data_for_missing_dates(conn, user_id, host_id, missing_dates):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∑–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞—Ç—ã"""
    if not missing_dates:
        print("üéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É–∂–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã! –ù–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –¥–∞—Ç.")
        return
    
    device_types = ['DESKTOP', 'MOBILE', 'TABLET']
    total_data = []
    
    print(f"\nüîÑ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –∑–∞ {len(missing_dates)} –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –¥–∞—Ç...")
    
    for i, target_date in enumerate(missing_dates, 1):
        print(f"\nüìÖ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞—Ç—É {i}/{len(missing_dates)}: {target_date}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ URL –¥–ª—è —ç—Ç–æ–π –¥–∞—Ç—ã
        urls = get_all_urls_for_date(user_id, host_id, target_date)
        print(f"   üìç –ù–∞–π–¥–µ–Ω–æ URL: {len(urls)}")
        
        if not urls:
            print(f"   ‚ö†Ô∏è –ù–µ—Ç URL —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞ {target_date}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
        
        date_data = []
        total_combinations = len(urls) * len(device_types)
        processed = 0
        
        for url in urls:
            for device in device_types:
                processed += 1
                if processed % 50 == 0:
                    print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed}/{total_combinations}")
                
                url_data = get_data_for_date_and_url(user_id, host_id, target_date, url, device)
                date_data.extend(url_data)
        
        total_data.extend(date_data)
        print(f"   ‚úÖ –°–æ–±—Ä–∞–Ω–æ –¥–∞–Ω–Ω—ã—Ö –∑–∞ {target_date}: {len(date_data)} –∑–∞–ø–∏—Å–µ–π")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    if total_data:
        df = pd.DataFrame(total_data)
        df = df.drop_duplicates()
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –¥–∞–Ω–Ω—ã–µ –≤ –ø–æ—Ä—è–¥–æ–∫
        print("\nüîß –ü—Ä–∏–≤–æ–¥–∏–º –¥–∞–Ω–Ω—ã–µ –≤ –ø–æ—Ä—è–¥–æ–∫...")
        df['demand'] = df['demand'].astype('int')
        df['impressions'] = df['impressions'].astype('int')
        df['clicks'] = df['clicks'].astype('int')
        df['position'] = df['position'].astype('float')
        df['device'] = df['device'].str.lower()
        
        print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–∞—Ç: {len(missing_dates)}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {df['page_path'].nunique()}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {df['query'].nunique()}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        save_to_database(df, conn)
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –¥–∞—Ç—ã")

def main():
    print(f"\n{'='*60}")
    print("–ò–ù–ö–†–ï–ú–ï–ù–¢–ê–õ–¨–ù–´–ô –°–ë–û–† –î–ê–ù–ù–´–• –ò–ó YANDEX WEBMASTER API")
    print(f"{'='*60}")
    
    # –ü–æ–ª—É—á–∞–µ–º user_id
    user_info_url = f"{BASE_URL}/user"
    try:
        response = requests.get(user_info_url, headers=headers)
        if response.status_code != 200:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {response.status_code}")
            print(f"   –¢–µ–∫—Å—Ç –æ—à–∏–±–∫–∏: {response.text}")
            return
        
        user_data = response.json()
        user_id = user_data['user_id']
        host_id = os.getenv('HOST_ID')
        
        print(f"üë§ User ID: {user_id}")
        print(f"üåê Host ID: {host_id}")
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
        print("\nüóÑÔ∏è –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
        try:
            conn = psycopg2.connect(**DB_CONFIG)
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞—Ç—ã (–ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –¥–Ω–µ–π)
            missing_dates = get_missing_dates(conn, user_id, host_id, days_back=20)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞—Ç—ã
            load_data_for_missing_dates(conn, user_id, host_id, missing_dates)
            
            conn.close()
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î –∑–∞–∫—Ä—ã—Ç–æ")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö: {e}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ API: {e}")

if __name__ == "__main__":
    main()
